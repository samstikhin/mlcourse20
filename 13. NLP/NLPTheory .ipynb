{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План теории:\n",
    "- Много вводных слов о важности NLP и сложности NLP задач \n",
    "- Эмбеддинги (разные виды эмбеддингов)\n",
    "- Кросс-энтропия. (как метрика)\n",
    "- Визуализация векторов слов\n",
    "- tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Введение в NLP\n",
    "Говорим о том, что люди общаются на естественном языке не понятному для машины. В современном мире есть потребность в том, чтобы вычислительные системы имели возможность принимать речь на естественном языке. Речь как письменную, так и устную. Или генерировать речь опять же как письменную, так и устную и ещё множество других задач, связанных в том или ином роде с обработкой естественного языка. Все такие проблемы и методы их решения объединены в такой области науки как NLP(Natural Language Processing). Более формально NLP - область искусственного интеллекта и математической лингвистики, в которой изучается проблема компьютерного анализа и синтеза естественного языка. \n",
    "\n",
    "На самом деле задачи NLP окружает нас повсюду, это задачи разной степени сложности. давайте рассмотрим несколько примеров. \n",
    "\n",
    "NLP в вашем смартфоне: возьмите свой смартфон, откройте любой мессенджер и начните набирать текст и вы увидите как в пенале над клавиатурой  подсистема вашей ос начнёт предлагать вам варианты слов которыми может быть продолжено ваше сообщение. Как же смартфон пытается предугадать ваше сообщение?[тут бы референс, статью например про такую систему умной клавиатуры, желательно без математики] \n",
    "\n",
    "NLP в вашей почте. Вы любите спам в почте? Никто не любит спам в почте. Как же сервисы электронной почты такие как Gmail.com, Яндекс.Почта, Mail.ru и т.д понимают, что некоторое письмо является спамом и этот источник писем нужно заблокировать ? Решение опять же кроиться сфере NLP. [тут тоже надо найти рефреренс, желательно не сложную статью, без математики]\n",
    "\n",
    "Чат боты. Вы, наверное, сталкивались с чат ботами, к примеру если вы пользовались telegramm, возможно вы общались с ботами, которые могли воспринимать не только структурированные команды но и сообщения в свободной форме на естественном языке и возможно даже могли грамотно отвечать на них, вот примеры таких ботов [Тут бы примеры ботов которые умею генерить текст]. А возможно вы общались с техподдержкой  какой-либо крупной компании, и встречали ботов там. Как правило такие боты призваны снять нагрузку с техподдержки  и уменьшить расходы на неё .В зависимости от канала связи чат боты могут быть как текстовыми так и речевыми. Часто вам могут звонить  из банка и предлагать услугу, и вы можете не подозревать что с вами говорит бот[рефреренсы на текстовых чат ботов и голосовых]\n",
    "\n",
    "Голосовые помощники: Активное развитие в последнее десятилетие получили голосовые помощники такие как Алиса от Яндекс, Сири от Apple, Кортана от Google, Алекса от Amazon. Такие системы способны анализировать вашу речь и грамотно реагировать на них, выполнять некоторые действия от таких как настройка будильника, таймера  и т.п до воспроизведение музыки по вашему запросу, заказу такси или доставки еды., и простая человеческая болтовня [тут референсы про голосовые помощники]\n",
    "\n",
    "\n",
    "Искусство: А что на счёт искусства? Так ребята из Яндекса использовали нейронную сеть чтобы сгенерировать песни, тексты которых схожи с текстами Егора Летова[тут референс], а позже и исполнили эти песни сами[рефреренс на нейроную оборону]. Тут, наверное, ещё есть примеры, надо погуглить.\n",
    "\n",
    "Так как NLP это область искуственного интелекта то тут нашли своё применение многие алгоритмы машинного обучения.  \n",
    "\n",
    "Этим примеры это малая доля того где используется NLP. Надеюсь, вы заинтересованы и мотивированы чтобы начать изучение NLP. Давайте начинать! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Эмбеддинги (разные виды эмбеддингов)\n",
    "\n",
    "Как мы  уже говорили ранее люди общаются на естественном языке, семантической единицей общения (т.е наименьшим элементом который несёт для нас какой либо смысл) является слово, более сложной семантической структурой является предложения как конечный набор слов. И первая проблема перед которой встаёт NLP это то как кодировать слово и смысл слова в пригодный для вычислений на ЭВМ формат. Представление слова в виде вектора действительных чисел называется word embedding. И в этом разделе мы поговорим о том как NLP решает задачу word embedding'а. \n",
    "\n",
    "Пусть нам дан некоторый текст на английском языке. Пусть это будет предложение I very love NLP. Давайте попробуем представить каждое слово нашего текста как некоторый вектор.\n",
    "\n",
    "Давайте разобьём I very love NLP на слова, каждое слово будем называть токеном. Получим последовательность токенов.\n",
    "\n",
    "I, very, love, nlp. и каждому токену сопоставим вектор.\n",
    "\n",
    "1 0 0 0 - I\n",
    "0 1 0 0 - very\n",
    "0 0 1 0 - love\n",
    "0 0 0 1 - nlp\n",
    "\n",
    "Ого, посмотрите мы получили эмбединги слов нашего предложения.\n",
    "На самом деле ясно что мы просто непросто взяли вектор размерность которого равна количеству уникальных слов тексте и каждой компоненте сопоставили уникальное слово.  С чем-то похожи вы должны были встречаться в блоке feature extracting. Такой подход называется one-hot embedding, он крайне прост, но обладает несколькими недостатками. Давайте в будущем все уникальные слова некоторого текста называть его словарём.  Главный недостаток one-hot embedding большая размерность вектора при большом словаре. Если в вашем тексте размерность словаря 1000000 слов, а слов в самом тексте очевидно больше, то потребуется большой объём памяти чтобы хранить эмбединги этих слов. Вторая проблема что такой разряженный вектор ничего вам не говорит о смысле слова и о том как связаны между собой семантически слова из текста. \n",
    "\n",
    "Например для текста:\n",
    "I eat apple, but not orange \n",
    "\n",
    "apple = 0 0 1 0 0 0\n",
    "orange = 0 0 0 0 0 1\n",
    "\n",
    "имея такие вектора нам трудно понять что слова за ними  скрывающиеся, относиться к фруктам или продуктам питания. А для задач  NLP, как мы видели, смысл слов очень важен.\n",
    "\n",
    "Мы с вами уже несколько раз говорили о смысле слова, но что такое смысл слова, давайте определим смысл в контексте nlp, это поможет нам более качественно решить проблему word embedding, ведь мы хотим отражать в действительных числах именно смысл слов! \n",
    "\n",
    "Пусть у нас есть предложения, в некотором контексте:\n",
    "\n",
    "1) Не садитесь за руль если употребили _________ \n",
    "2) бутылка ___ стояла на столе\n",
    "3) ___ вредит здоровью \n",
    "4) ___ делается из винограда\n",
    "\n",
    "И у нас есть 3 слова 1) Алкоголь, 2) молоко, 3) вино. Построим следующую таблицу\n",
    "\n",
    "О каком из этих 3 слов идёт речь.\n",
    "вы подумали что речь про вино или про алкоголь, эту информацию вы взяли из контекста предложений, иначе это можно записать в виде таблиц. \n",
    "         Контекст 1 Контекст 2 Контекст 3 Контекст 4\n",
    "Алкоголь 1 1 1 0\n",
    "Молоко   0 1 0 0\n",
    "Вино     1 1 1 1\n",
    "\n",
    "Ого и это тоже эмбединг слов, и в нём мы уже видим что векторы для слова алкоголи и вино похоже друг на друга больше чем  молоко с любым из них. \n",
    "\n",
    "Мы предположим, что слова, которые встречаются в одинаковых контекстах имеют одинаковый смысл, т.е если мы заменим одно слова на другое то контекст предложения сильно не измениться. Иначе говоря, мы будем считать, что смысл слова — это то в каких контекстах(или с каким словами рядом) оно встречается. Такое предположение называется дистрибутивной гипотезой. \n",
    "\n",
    "Теперь мы поняли что такое смысл и как определять смысл слова, давайте научим компьютер делать тоже самое. \n",
    "\n",
    "Пусть у нас есть некоторый набор документов S = (s_1, s_2, s_3, ... s_n), понятно что каждый документ это не пустое множество слов. S - будем называть корпусом. Voc(S) - словарь над корпусом слов. Давайте каждое слова w из Voc(S) представим вектором размерность которого |Voc(S)|, компонента это слово w' из Voc(S), а значение компоненты это то сколько раз w встречалось рядом с w' в корпусе S. Например:\n",
    "\n",
    "ТУТ ДОБАВИТЬ ПРИМЕР \n",
    "\n",
    "Получаем огромную матрицу. Понятно, что у неё есть те же проблемы что и у one-hot embedding подхода, однако понятно что мы можем уже определять похожие друг на друга по смыслу слова в зависимости от того как часто они встречаются рядом. Ведь если w встречается часто с w' то и w' встречается рядом с w(надо отразить это в примере), и вектора слов будут похожи. Однако проблему большой размерности вектора удалось решить многими способами, в том числе и с помощью математики, но об этом мы поговорим позже. Сейчас для нас важно понимать, что есть способы нормализовать строки в получившейся матрице и уменьшить её размерность \n",
    "\n",
    "\n",
    "Тут переход на Word2Vec.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
