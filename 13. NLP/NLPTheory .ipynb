{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## План теории:\n",
    "- Много вводных слов о важности NLP и сложности NLP задач \n",
    "- Эмбеддинги (разные виды эмбеддингов)\n",
    "- Кросс-энтропия. (как метрика)\n",
    "- Визуализация векторов слов\n",
    "- tf-idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Введение в NLP\n",
    "\n",
    "Говорим о том, что люди общаются на естественном языке не понятному для машины. В современном мире есть потребность в том, чтобы вычислительные системы имели возможность принимать речь на естественном языке. Речь как письменную, так и устную. Или генерировать речь опять же как письменную, так и устную и ещё множество других задач, связанных в том или ином роде с обработкой естественного языка. Все такие проблемы и методы их решения объединены в такой области науки как NLP(Natural Language Processing). Более формально NLP - область искусственного интеллекта и математической лингвистики, в которой изучается проблема компьютерного анализа и синтеза естественного языка. \n",
    "\n",
    "На самом деле задачи NLP окружает нас повсюду, это задачи разной степени сложности. давайте рассмотрим несколько примеров. \n",
    "\n",
    "NLP в вашем смартфоне: возьмите свой смартфон, откройте любой мессенджер и начните набирать текст и вы увидите как в пенале над клавиатурой  подсистема вашей ос начнёт предлагать вам варианты слов которыми может быть продолжено ваше сообщение. Как же смартфон пытается предугадать ваше сообщение?[тут бы референс, статью например про такую систему умной клавиатуры, желательно без математики] \n",
    "\n",
    "NLP в вашей почте. Вы любите спам в почте? Никто не любит спам в почте. Как же сервисы электронной почты такие как Gmail.com, Яндекс.Почта, Mail.ru и т.д понимают, что некоторое письмо является спамом и этот источник писем нужно заблокировать ? Решение опять же кроиться сфере NLP. [тут тоже надо найти рефреренс, желательно не сложную статью, без математики]\n",
    "\n",
    "Чат боты. Вы, наверное, сталкивались с чат ботами, к примеру если вы пользовались telegramm, возможно вы общались с ботами, которые могли воспринимать не только структурированные команды но и сообщения в свободной форме на естественном языке и возможно даже могли грамотно отвечать на них, вот примеры таких ботов [Тут бы примеры ботов которые умею генерить текст]. А возможно вы общались с техподдержкой  какой-либо крупной компании, и встречали ботов там. Как правило такие боты призваны снять нагрузку с техподдержки  и уменьшить расходы на неё .В зависимости от канала связи чат боты могут быть как текстовыми так и речевыми. Часто вам могут звонить  из банка и предлагать услугу, и вы можете не подозревать что с вами говорит бот[рефреренсы на текстовых чат ботов и голосовых]\n",
    "\n",
    "Голосовые помощники: Активное развитие в последнее десятилетие получили голосовые помощники такие как Алиса от Яндекс, Сири от Apple, Кортана от Google, Алекса от Amazon. Такие системы способны анализировать вашу речь и грамотно реагировать на них, выполнять некоторые действия от таких как настройка будильника, таймера  и т.п до воспроизведение музыки по вашему запросу, заказу такси или доставки еды., и простая человеческая болтовня [тут референсы про голосовые помощники]\n",
    "\n",
    "\n",
    "Искусство: А что на счёт искусства? Так ребята из Яндекса использовали нейронную сеть чтобы сгенерировать песни, тексты которых схожи с текстами Егора Летова[тут референс], а позже и исполнили эти песни сами[рефреренс на нейроную оборону]. Тут, наверное, ещё есть примеры, надо погуглить.\n",
    "\n",
    "Так как NLP это область искуственного интелекта то тут нашли своё применение многие алгоритмы машинного обучения.  \n",
    "\n",
    "Этим примеры это малая доля того где используется NLP. Надеюсь, вы заинтересованы и мотивированы чтобы начать изучение NLP. Давайте начинать! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Эмбеддинги (разные виды эмбеддингов)\n",
    "\n",
    "Как мы  уже говорили ранее люди общаются на естественном языке, семантической единицей общения (т.е наименьшим элементом который несёт для нас какой либо смысл) является слово, более сложной семантической структурой является предложения как конечный набор слов. И первая проблема перед которой встаёт NLP это то как кодировать слово и смысл слова в пригодный для вычислений на ЭВМ формат. Представление слова в виде вектора действительных чисел называется word embedding. И в этом разделе мы поговорим о том как NLP решает задачу word embedding'а. \n",
    "\n",
    "Пусть нам дан некоторый текст на английском языке. Пусть это будет предложение I very love NLP. Давайте попробуем представить каждое слово нашего текста как некоторый вектор.\n",
    "\n",
    "Давайте разобьём I very love NLP на слова, каждое слово будем называть токеном. Получим последовательность токенов.\n",
    "\n",
    "### One-hot embedding\n",
    "I, very, love, nlp. и каждому токену сопоставим вектор.<br>\n",
    "\n",
    "1 0 0 0 - I<br>\n",
    "0 1 0 0 - very<br>\n",
    "0 0 1 0 - love<br>\n",
    "0 0 0 1 - nlp<br>\n",
    "\n",
    "Ого, посмотрите мы получили эмбединги слов нашего предложения.\n",
    "На самом деле ясно что мы просто непросто взяли вектор размерность которого равна количеству уникальных слов тексте и каждой компоненте сопоставили уникальное слово.  С чем-то похожи вы должны были встречаться в блоке feature extracting. Такой подход называется one-hot embedding, он крайне прост, но обладает несколькими недостатками. Давайте в будущем все уникальные слова некоторого текста называть его словарём.  Главный недостаток one-hot embedding большая размерность вектора при большом словаре. Если в вашем тексте размерность словаря 1000000 слов, а слов в самом тексте очевидно больше, то потребуется большой объём памяти чтобы хранить эмбединги этих слов. Вторая проблема что такой разряженный вектор ничего вам не говорит о смысле слова и о том как связаны между собой семантически слова из текста. \n",
    "\n",
    "Например для текста:\n",
    "I eat apple, but not orange<br>\n",
    "apple = 0 0 1 0 0 0<br>\n",
    "orange = 0 0 0 0 0 1<br>\n",
    "\n",
    "имея такие вектора нам трудно понять что слова за ними  скрывающиеся, относиться к фруктам или продуктам питания. А для задач  NLP, как мы видели, смысл слов очень важен.\n",
    "\n",
    "Мы с вами уже несколько раз говорили о смысле слова, но что такое смысл слова, давайте определим смысл в контексте nlp, это поможет нам более качественно решить проблему word embedding, ведь мы хотим отражать в действительных числах именно смысл слов! \n",
    "\n",
    "### Про смысл\n",
    "Пусть у нас есть предложения, в некотором контексте:\n",
    "\n",
    "1. Не садитесь за руль если употребили _________ \n",
    "\n",
    "2. бутылка ___ стояла на столе\n",
    "\n",
    "3. ___ вредит здоровью \n",
    "\n",
    "4. ___ делается из винограда\n",
    "\n",
    "И у нас есть 3 слова 1) Алкоголь, 2) молоко, 3) вино. Построим следующую таблицу\n",
    "\n",
    "О каком из этих 3 слов идёт речь.\n",
    "Вы подумали что речь про вино или про алкоголь, эту информацию вы взяли из контекста предложений. Формально контекстом мы будем называть некоторое количество слов слева и справа от того для которого мы пытаемся определить контекст. Это так потому что когда вы пыталсь определить загаданное слово во всех 4 предложениях вы опиралсь именно на те слова которые стоят рядом. \n",
    "\n",
    "запишем в табличку. \n",
    "\n",
    "            Контекст 1 Контекст 2 Контекст 3 Контекст 4\n",
    "    Алкоголь 1            1         1           0\n",
    "    Молоко   0            1         0           0\n",
    "    Вино     1            1         1           1\n",
    "\n",
    "\n",
    "Ого и это тоже эмбединг слов, и в нём мы уже видим что векторы для слова алкоголи и вино похоже друг на друга больше чем  молоко с любым из них. \n",
    "\n",
    "Мы предположим, что слова, которые встречаются в одинаковых контекстах имеют одинаковый смысл, т.е если мы заменим одно слова на другое то контекст предложения сильно не измениться. Иначе говоря, мы будем считать, что смысл слова — это то в каких контекстах(или с каким словами рядом) оно встречается. Такое предположение называется дистрибутивной гипотезой. \n",
    "\n",
    "Теперь мы поняли что такое смысл и как определять смысл слова, давайте научим компьютер делать тоже самое.\n",
    "\n",
    "### embedding через отношение рядом\n",
    "\n",
    "Пусть у нас есть некоторый набор документов S = (s_1, s_2, s_3, ... s_n), понятно что каждый документ это не пустое множество слов. S - будем называть корпусом. Voc(S) - словарь над корпусом слов.\n",
    "[тут задать S]\n",
    "\n",
    "\n",
    "Давайте каждое слова w из Voc(S) представим вектором размерность которого |Voc(S)|, компонента это слово w' из Voc(S), а значение компоненты это то сколько раз w встречалось рядом с w' в корпусе S. Например:\n",
    "\n",
    "[тут пример]\n",
    "\n",
    "Получаем огромную матрицу размерностью |Voc(S)|x|Voc(S)|. Понятно, что у неё есть те же проблемы что и у one-hot embedding подхода, мы получаем огромную разряженную матрицу. Однако понятно что мы можем уже определять похожие друг на друга по смыслу слова в зависимости от того как часто они встречаются рядом. Ведь если w встречается часто с w' то и w' встречается рядом с w(надо отразить это в примере), и вектора слов будут похожи.\n",
    "\n",
    "### LSA\n",
    "\n",
    "Давайте рассмотрим другой метод embeddinga он похож на тот что мы привели выше, однако имеет приемущество: он способен решить проблему большой размерности и разряженности векторов. Этот метода называется LSA или Latent semantic analysis. LSA основывается исключительно на хорошо знакомой всем нам линейно алгебре, мы не будем подобно углублятся в устройство работы этого метода, все страждующие смогут почитать [тут].\n",
    "\n",
    "Перейдём непосредственно к самому методу. Теперь мы будем брать слова w из Voc(S) и считать сколько раз w встретилось в документе s_i. Понятно что в том случае мы получим таблицу размерностю |S|x|Voc(S)| назовём её M.\n",
    "\n",
    "[тут пример]\n",
    "\n",
    "Полсе этого мы прменяем такую магически-математическую штуку как SVD. SVD разбивает нашу матрицу M на произведение трёх матрицу M = U\\*S\\*V. Эти три матрицы обладают интересными свойствами. U и V это ортогональные друг другу матрицы, а S - диагональная матрица, и элементы диагонали упорядоченны в порядке убывания и называются сингулярынми значениями. Давайте рассмотрим пример:\n",
    "\n",
    "[тут пример разложения]\n",
    "\n",
    "Вся магия заключается в том, что строки матрицы U ничто иное как embeddig'и слов из |Voc(S)|. А S обладает замечательным свойством: строки из U соответсвующие наименьшиму сингулярнму значению дают наименьший вклад в итоговое произведение.  Т.е  сингулярное значение является оценкой смысла слова. Получается что если k наименьших значений S не сильно влияют на результат произведения, то их можно занулить, тогда нет смысла рассмтривать k последних столбцов матрицы U. Иными словами мы можем выкинуть из расмотрения k слов из Voc(S) так как их смысл в масштабе всего корпуса крайне мал. \n",
    "\n",
    "Таким образом мы получим матрицу эмбеддигов, размерность которой будет |Voc(S)| - k строк на |S| столбцов. Где строки - это эмбеддинги слов из |Voc(S)| за исключением тех k что соответсвовали наименьшим сингулярым значениям. \n",
    "\n",
    "[тут матрица]\n",
    "\n",
    "LSA решает проблему большой разряженной матрицы, однако давайте подумаем что собственно будет если мы захотим со временем добавить к нашим эмбеддингам новые слова? Конечно, всё нужно пересчитывать заново, и вот мы плавно подошли к венцу word embedding встречайте Word2Vec!\n",
    "\n",
    "### WORD2VEC \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
